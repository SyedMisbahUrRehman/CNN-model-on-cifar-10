# -*- coding: utf-8 -*-
"""alexanet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rUDUSjHKsEQi0zZ-Ymk94UV1ITCzOm-I
"""

#AlexNet architecture
import os
import keras
from keras.datasets import cifar10
from keras.layers import Input, Conv2D, GlobalAveragePooling2D, Dense, BatchNormalization, Activation, MaxPooling2D
from keras.models import Model
from keras.layers import Dropout, Flatten
from keras import optimizers
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import LearningRateScheduler, TensorBoard
import matplotlib.pyplot as plt

num_classes = 10
batch_size = 64
epochs = 30
iterations = 780
DROPOUT = 0.5
CONCAT_AXIS = 3
DATA_FORMAT = 'channels_last'
log_filepath = 'C:\\Users\\Esrat Maria\\Desktop\\AlexNet'

def color_preprocessing(x_train, x_test):
    x_train = x_train.astype('float32')
    x_test = x_test.astype('float32')
    mean = [125.307, 122.95, 113.865]
    std = [62.9932, 62.0887, 66.7048]
    for i in range(3):
        x_train[:, :, :, i] = (x_train[:, :, :, i] - mean[i]) / std[i]
        x_test[:, :, :, i] = (x_test[:, :, :, i] - mean[i]) / std[i]
    return x_train, x_test

# defining learning rate based on the number of epoch
def scheduler(epoch):
    if epoch < 100:
        return 0.01
    if epoch < 200:
        return 0.001
    return 0.0001

# loading cifar10 data
(x_train, y_train), (x_test, y_test) = cifar10.load_data()
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)
x_train, x_test = color_preprocessing(x_train, x_test)

# Building CNN (AlexNet)
def alexnet(img_input, classes=10):

  # 1st conv layer
  x = Conv2D(96, (11, 11), strides=(4, 4), padding='same',
             activation='relu', kernel_initializer='uniform')(img_input)  # valid
  x = MaxPooling2D(pool_size=(3, 3), strides=(
      2, 2), padding='same', data_format=DATA_FORMAT)(x)
  x = BatchNormalization()(x)

  # 2nd conv layer
  x = Conv2D(256, (5, 5), strides=(1, 1), padding='same',
             activation='relu', kernel_initializer='uniform')(x)
  x = MaxPooling2D(pool_size=(3, 3), strides=(
      2, 2), padding='same', data_format=DATA_FORMAT)(x)
  x = BatchNormalization()(x)

  # 3rd conv layer
  x = Conv2D(384, (3, 3), strides=(1, 1), padding='same',
             activation='relu', kernel_initializer='uniform')(x)
  x = BatchNormalization()(x)

  # 4th conv layer
  x = Conv2D(384, (3, 3), strides=(1, 1), padding='same',
             activation='relu', kernel_initializer='uniform')(x)
  x = BatchNormalization()(x)

  # 5th conv layer
  x = Conv2D(256, (3, 3), strides=(1, 1), padding='same',
             activation='relu', kernel_initializer='uniform')(x)
  x = MaxPooling2D(pool_size=(3, 3), strides=(
      2, 2), padding='same', data_format=DATA_FORMAT)(x)
  x = BatchNormalization()(x)

  # flattening before sending to fully connected layers
  x = Flatten()(x)
  # fully connected layers
  x = Dense(4096, activation='relu')(x)
  x = Dropout(0.5)(x)
  x = BatchNormalization()(x)
  x = Dense(1000, activation='relu')(x)
  x = Dropout(0.5)(x)
  x = BatchNormalization()(x)

  # output layer
  out = Dense(classes, activation='softmax')(x)
  return out


# defining input image size according to cifar10
img_input = Input(shape=(32, 32, 3))
output = alexnet(img_input)
model = Model(img_input, output)
model.summary()

# First Learning Rate (lr=.001)
sgd_lr_001 = optimizers.SGD(lr=.001, momentum=0.9, nesterov=True)
model.compile(loss='categorical_crossentropy',
              optimizer=sgd_lr_001, metrics=['accuracy'])
tb_cb_lr_001 = TensorBoard(log_dir=log_filepath + '_lr_001', histogram_freq=0)
change_lr_lr_001 = LearningRateScheduler(scheduler)
cbks_lr_001 = [change_lr_lr_001, tb_cb_lr_001]

datagen.fit(x_train)

history_lr_001 = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),
                                     steps_per_epoch=iterations,
                                     epochs=epochs,
                                     callbacks=cbks_lr_001,
                                     validation_data=(x_test, y_test))

# plotting loss VS epoch for lr=.001
plt.plot(history_lr_001.history['loss'], label='Loss (lr=.001)')
plt.plot(history_lr_001.history['val_loss'], label='val_loss (lr=.001)')
plt.title('Model Loss (lr=.001)')
plt.ylabel('Training and Test Loss')
plt.xlabel('Epoch')
plt.legend(['Train (lr=.001)', 'Validation (lr=.001)'], loc='upper left')
plt.show()

# Second Learning Rate (lr=.01)
sgd_lr_01 = optimizers.SGD(lr=.01, momentum=0.9, nesterov=True)
model.compile(loss='categorical_crossentropy',
              optimizer=sgd_lr_01, metrics=['accuracy'])
tb_cb_lr_01 = TensorBoard(log_dir=log_filepath + '_lr_01', histogram_freq=0)
change_lr_lr_01 = LearningRateScheduler(scheduler)
cbks_lr_01 = [change_lr_lr_01, tb_cb_lr_01]

history_lr_01 = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),
                                    steps_per_epoch=iterations,
                                    epochs=epochs,
                                    callbacks=cbks_lr_01,
                                    validation_data=(x_test, y_test))

# plotting loss VS epoch for lr=.01
plt.plot(history_lr_01.history['loss'], label='Loss (lr=.01)')
plt.plot(history_lr_01.history['val_loss'], label='val_loss (lr=.01)')
plt.title('Model Loss (lr=.01)')
plt.ylabel('Training and Test Loss')
plt.xlabel('Epoch')
plt.legend(['Train (lr=.01)', 'Validation (lr=.01)'], loc='upper left')
plt.show()

# Display final test accuracies
print('Loading the final test accuracies...')
test_loss_lr_001, test_acc_lr_001 = model.evaluate(x_test, y_test, verbose=2)
test_loss_lr_01, test_acc_lr_01 = model.evaluate(x_test, y_test, verbose=2)

# Final accuracies
print('Final Test Accuracy (lr=.001):', test_acc_lr_001)
print('Final Test Accuracy (lr=.01):', test_acc_lr_01)

